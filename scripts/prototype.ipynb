{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from typing import Final, Iterator, Sequence\n",
    "\n",
    "import polars as pl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from lxml import etree as et\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "ROOT: Final[str] = \"https://www.metacritic.com/game\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_sitemap_index(s: requests.Session) -> int:\n",
    "    \"\"\"Return index value of the last sitemap in the sitemap catalog.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemap catalog over a persistent http connection.\n",
    "    \"\"\"\n",
    "    sitemap_catalog = s.get(f\"{ROOT}s.xml\")\n",
    "    sitemap_catalog.raise_for_status()\n",
    "    catalog_etree = et.XML(sitemap_catalog.text)\n",
    "    nsmap = {\"ns\": catalog_etree.nsmap[None]}\n",
    "    # XPath query tested faster compared to ElementPath methods\n",
    "    last_url = catalog_etree.xpath(\n",
    "        \"(//ns:loc/text())[last()]\",\n",
    "        namespaces=nsmap,\n",
    "        smart_strings=False,\n",
    "    )\n",
    "    if not last_url:\n",
    "        raise IndexError(\"No sitemaps found.\")\n",
    "    last_index = int(last_url[0].rpartition(\"/\")[-1].partition(\".\")[0])\n",
    "    return last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_slugs(loc_elements: Iterator[et._Element], url_slugs: set[str]) -> None:\n",
    "    \"\"\"Process a single sitemap's loc tags and update the set of URL slugs.\n",
    "\n",
    "    Args:\n",
    "        loc_elements: A generator of loc tags containing URL text.\n",
    "        url_slugs: The set in which game URL slugs are to be stored.\n",
    "    \"\"\"\n",
    "    for loc in loc_elements:\n",
    "        if (url := loc.text) is None:\n",
    "            continue\n",
    "        url_slug = url.rstrip(\"/\").rpartition(\"/\")[-1]\n",
    "        url_slugs.add(url_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sitemaps(s: requests.Session, last_index: int) -> list[str]:\n",
    "    \"\"\"Retrieve URL slugs associated with all games from sitemaps.\n",
    "\n",
    "    Colloquially, a slug is the unique identifying part of a web address,\n",
    "    typically at the end of the URL. Each sitemap contains approximately 1000 URLs.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemaps over a persistent http connection.\n",
    "        last_index: End value for iteration of all sitemaps. (There appear to\n",
    "            be more unlisted sitemaps of higher indices in the catalog,\n",
    "            but these appear to be duplicates or vestigial in nature).\n",
    "\n",
    "    Returns:\n",
    "        A list of URL slugs corresponding to every game indexed in metacritic's sitemaps.\n",
    "    \"\"\"\n",
    "    url_slugs: set[str] = set()\n",
    "    for i in range(1, last_index + 1):\n",
    "        sitemap_response = s.get(f\"{ROOT}s/{i}.xml\")\n",
    "        sitemap_response.raise_for_status()\n",
    "        sitemap_etree = et.XML(sitemap_response.text)\n",
    "        loc_elements = sitemap_etree.iterfind(\".//loc\", namespaces=sitemap_etree.nsmap)\n",
    "        extract_url_slugs(loc_elements, url_slugs)\n",
    "\n",
    "    return list(url_slugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_game_details(game_pg: requests.Response):\n",
    "    soup = BeautifulSoup(game_pg.content, features=\"lxml\")\n",
    "    detail_tags = soup.select(\n",
    "        \"meta[name='description'], \"\n",
    "        \".c-productHero_title > div, \"\n",
    "        \".c-productHero_scoreInfo .c-siteReviewScore > span, \"\n",
    "        \".c-productionDetailsGame_esrb_title > span:first-child, \"\n",
    "        \".c-gameDetails_Platforms > ul, \"\n",
    "        \".c-gameDetails_ReleaseDate :last-child, \"\n",
    "        \".c-gameDetails_Developer ul, \"\n",
    "        \".c-gameDetails_Distributor :last-child, \"\n",
    "        \"ul.c-genreList\"\n",
    "    )\n",
    "    processed_details: list[str | list[str]] = []\n",
    "    for i, tag in enumerate(detail_tags):\n",
    "        # Special case as game description text is only found in full as an attribute value.\n",
    "        if i == 0:\n",
    "            processed_details.append(\n",
    "                str(tag[\"content\"]).replace(\"\\t\", \"\").replace(\"\\n\", \" \")\n",
    "            )\n",
    "        elif i in [1, 2, 3, 4, 6, 8]:\n",
    "            stripped = tag.get_text(strip=True)\n",
    "            # Scale Userscore to int range for later memory optimization\n",
    "            if i == 3:\n",
    "                processed_details.append(str(int(10 * float(stripped))))\n",
    "            # Remove unnecessary \"Rated \" text preceding esrb label\n",
    "            elif i == 4:\n",
    "                processed_details.append(stripped.partition(\" \")[-1])\n",
    "            else:\n",
    "                processed_details.append(stripped)\n",
    "        # Tags containing multiple children\n",
    "        elif i in [5, 7, 9]:\n",
    "            processed_details.append([li.get_text(strip=True) for li in tag])\n",
    "    processed_details.append(processed_details.pop(0))\n",
    "    with open(\"../data/game_details.tsv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=\"\\t\")\n",
    "        writer.writerow(processed_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_info():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_games(s: requests.Session, url_slugs: list[str]):\n",
    "    \"\"\" \"\"\"\n",
    "    game_url = f\"{ROOT}/elden-ring\"\n",
    "    game_pg = s.get(game_url)\n",
    "    if not game_pg.ok and game_pg.status_code != 429:\n",
    "        print(\"error fetching game page\")\n",
    "        return\n",
    "        # continue\n",
    "    critic_pg = s.get(f\"{game_url}/critic-reviews\")\n",
    "    scrape_game_details(game_pg)\n",
    "    scrape_critic_info()\n",
    "\n",
    "\n",
    "with requests.Session() as s:\n",
    "    s.headers = {\"User-Agent\": \"Edge\"}\n",
    "    scrape_games(s, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(\n",
    "    user_agent: str = \"Edge\",\n",
    "    num_retries: int = 5,\n",
    "    backoff_factor: float = 0.5,\n",
    "    backoff_jitter: float = 0.5,\n",
    "    status_forcelist: list[int] = [429],\n",
    "    respect_retry_after_header: bool = False,\n",
    ") -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    with requests.Session() as s:\n",
    "        s.headers = {\"User-Agent\": user_agent}\n",
    "        retry_config = Retry(\n",
    "            total=num_retries,\n",
    "            backoff_factor=backoff_factor,\n",
    "            backoff_jitter=backoff_jitter,\n",
    "            status_forcelist=status_forcelist,\n",
    "            respect_retry_after_header=respect_retry_after_header,\n",
    "        )\n",
    "        s.mount(\"https://\", HTTPAdapter(max_retries=retry_config))\n",
    "        if not os.path.isfile(\"../data/games.tsv\"):\n",
    "            last_index = get_last_sitemap_index(s)\n",
    "            url_slugs = scrape_sitemaps(s, last_index)\n",
    "            pl.DataFrame({\"url_slugs\": url_slugs}).write_csv(\n",
    "                file=\"../data/games.tsv\",\n",
    "                separator=\"\\t\",\n",
    "            )\n",
    "        else:\n",
    "            url_slugs = pl.read_csv(\"../data/games.tsv\").to_series().to_list()\n",
    "        scrape_games(s, url_slugs)\n",
    "\n",
    "\n",
    "scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(\n",
    "    url_slugs: Sequence[str | int] = [], *, is_index: bool = False\n",
    ") -> str:\n",
    "    \"\"\"Return string adhereing to XML schema for the Sitemap protocol\n",
    "    containing user defined dummy urls.\n",
    "\n",
    "    Utility function to construct body content for mock http responses.\n",
    "    Any type of string is accepted, but only ints or int-like strings are\n",
    "    expected within the list if is_index is true (constructing a sitemap index).\n",
    "\n",
    "    Args:\n",
    "        url_slugs: Corresponds to game titles when constructing a sitemap, while\n",
    "            represents ordinal values when constructing a sitemap index.\n",
    "        is_index: Determines whether to construct a template for a sitemap\n",
    "            or a sitemap index.\n",
    "    \"\"\"\n",
    "    ns = 'xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"'\n",
    "    container_start_tag = f\"<urlset {ns}>\"\n",
    "    item_start_tag = \"<url><loc>\"\n",
    "    ext = \"/\"\n",
    "    item_end_tag = \"</loc></url>\"\n",
    "    container_end_tag = \"</urlset>\"\n",
    "    if is_index:\n",
    "        container_start_tag = f\"<sitemapindex {ns}>\"\n",
    "        item_start_tag = \"<sitemap><loc>\"\n",
    "        ext = \".xml\"\n",
    "        item_end_tag = \"</loc></sitemap>\"\n",
    "        container_end_tag = \"</sitemapindex>\"\n",
    "    # The empty string element is a special case where the url root is also omitted.\n",
    "    url_generator = (\n",
    "        (\n",
    "            f\"{item_start_tag}{item_end_tag}\"\n",
    "            if slug == \"\"\n",
    "            else f\"{item_start_tag}{ROOT}/{slug}{ext}{item_end_tag}\"\n",
    "        )\n",
    "        for slug in url_slugs\n",
    "    )\n",
    "    return f\"{container_start_tag}{\" \".join(url_generator)}{container_end_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "import responses\n",
    "\n",
    "\n",
    "class TestPrototype(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.s = cls.enterClassContext(requests.Session())\n",
    "        cls.r = cls.enterClassContext(responses.RequestsMock())\n",
    "\n",
    "    def test_get_last_sitemap_index(self) -> None:\n",
    "        statuses = [200] * 5 + [404]\n",
    "        slugs_to_test = [\n",
    "            [7],\n",
    "            [\"a\", 20],\n",
    "            [11, 3, 94],\n",
    "            [],\n",
    "            [20, \"a\"],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        for status, slugs in zip(statuses, slugs_to_test):\n",
    "            self.r.get(\n",
    "                url=f\"{ROOT}s.xml\",\n",
    "                status=status,\n",
    "                body=create_template(slugs, is_index=True),\n",
    "            )\n",
    "\n",
    "        for i in range(3):\n",
    "            self.assertEqual(get_last_sitemap_index(self.s), slugs_to_test[i][-1])\n",
    "        self.assertRaisesRegex(\n",
    "            IndexError, \"No sitemaps found.\", get_last_sitemap_index, self.s\n",
    "        )\n",
    "        self.assertRaises(ValueError, get_last_sitemap_index, self.s)\n",
    "        self.assertRaises(requests.HTTPError, get_last_sitemap_index, self.s)\n",
    "\n",
    "    def test_scrape_sitemaps(self) -> None:\n",
    "        statuses = [200, 200, 404]\n",
    "        slugs_to_test = [\n",
    "            [\"dark-summit\", \"warhawk\", \"dark-summit\"],\n",
    "            [\"\"],\n",
    "            [\"bioshock\"],\n",
    "        ]\n",
    "        for status, slugs in zip(statuses, slugs_to_test):\n",
    "            self.r.get(\n",
    "                url=f\"{ROOT}s/1.xml\",\n",
    "                status=status,\n",
    "                body=create_template(slugs),\n",
    "            )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s/2.xml\",\n",
    "            status=200,\n",
    "            body=create_template([\"warhawk\", \"metal-slug-2\"]),\n",
    "        )\n",
    "\n",
    "        TEST_INDEX = 2\n",
    "        self.assertCountEqual(\n",
    "            scrape_sitemaps(self.s, TEST_INDEX),\n",
    "            [\"dark-summit\", \"warhawk\", \"metal-slug-2\"],\n",
    "        )\n",
    "        self.assertCountEqual(\n",
    "            scrape_sitemaps(self.s, TEST_INDEX),\n",
    "            [\"warhawk\", \"metal-slug-2\"],\n",
    "        )\n",
    "        self.assertRaises(requests.HTTPError, scrape_sitemaps, self.s, TEST_INDEX)\n",
    "\n",
    "\n",
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

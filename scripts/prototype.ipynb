{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import cchardet\n",
    "import lxml\n",
    "import os\n",
    "import polars as pl\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from lxml import etree as et\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "ROOT = \"https://www.metacritic.com/game\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_slugs(loc_elements: list[et._Element], url_slugs: set[str]) -> None:\n",
    "    \"\"\"Process a single sitemap's loc tags and update the set of URL slugs.\n",
    "\n",
    "    Args:\n",
    "        loc_elements: A list of loc tags containing url text.\n",
    "        url_slugs: The set in which game url slugs are to be stored to.\n",
    "    \"\"\"\n",
    "    for loc in loc_elements:\n",
    "        if (url := loc.text) is None:\n",
    "            continue\n",
    "        url_slug = url.rstrip(\"/\").rpartition(\"/\")[-1]\n",
    "        url_slugs.add(url_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sitemaps(s: requests.Session) -> list[str]:\n",
    "    \"\"\"Retrieve URL slugs associated with all games from sitemaps.\n",
    "\n",
    "    Colloquially, a slug is the unique identifying part of a web address,\n",
    "    typically at the end of the URL. Each sitemap contains at most 100 urls.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemaps over a persistent http connection.\n",
    "\n",
    "    Returns:\n",
    "        A list of url slugs corresponding to every game indexed in metacritic's sitemaps.\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    url_slugs: set[str] = set()\n",
    "    while True:\n",
    "        sitemap_response = s.get(f\"{ROOT}s/{i}.xml\")\n",
    "        if not sitemap_response.ok:\n",
    "            print(f\"Sitemap page {i} failed to respond, skipped.\")\n",
    "            i += 1\n",
    "            continue\n",
    "        sitemap_tree = et.XML(sitemap_response.text)\n",
    "        loc_elements = sitemap_tree.findall(\".//{*}loc\")\n",
    "        if not loc_elements:\n",
    "            break\n",
    "        extract_url_slugs(loc_elements, url_slugs)\n",
    "        i += 1\n",
    "\n",
    "    return list(url_slugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_games(s: requests.Session, url_slugs: list[str]) -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(\n",
    "    root_url: str,\n",
    "    user_agent: str = \"Edge\",\n",
    "    max_retries: int = 5,\n",
    ") -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    with requests.Session() as s:\n",
    "        s.headers = {\"User-Agent\": user_agent}\n",
    "        retry_config = Retry(\n",
    "            total=max_retries,\n",
    "            backoff_factor=0.5,\n",
    "            backoff_jitter=0.5,\n",
    "            status_forcelist=[429],\n",
    "            respect_retry_after_header=False,\n",
    "        )\n",
    "        s.mount(\"https://\", HTTPAdapter(max_retries=retry_config))\n",
    "        if not os.path.isfile(\"../data/games.tsv\"):\n",
    "            url_slugs = scrape_sitemaps(s)\n",
    "            pl.DataFrame({\"url_slugs\": url_slugs}).write_csv(\n",
    "                file=\"../data/games.tsv\",\n",
    "                separator=\"\\t\",\n",
    "            )\n",
    "        else:\n",
    "            url_slugs = pl.read_csv(\"../data/games.tsv\").to_series().to_list()\n",
    "        scrape_games(s, url_slugs)\n",
    "\n",
    "\n",
    "scraper(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template_sitemap(url_slugs: list[str] = []) -> str:\n",
    "    \"\"\"Return string adhereing to XML schema for the Sitemap protocol\n",
    "    containing user defined dummy urls.\n",
    "\n",
    "    Utility function to construct body content for mock http responses.\n",
    "\n",
    "    Args:\n",
    "        url_slugs: List of game url slugs used in constructing each\n",
    "            url within the sitemap urlset.\n",
    "            Uses slugs instead of full urls because every game should\n",
    "            already share the same url root.\n",
    "    \"\"\"\n",
    "    start_tags = \"<url><loc>\"\n",
    "    end_tags = \"</loc><changefreq>weekly</changefreq></url>\"\n",
    "    # The empty string element is a special case where the url root is also omitted.\n",
    "    url_generator = (\n",
    "        f\"{start_tags}{end_tags}\"\n",
    "        if slug == \"\"\n",
    "        else f\"{start_tags}{ROOT}/{slug}/{end_tags}\"\n",
    "        for slug in url_slugs\n",
    "    )\n",
    "    return (\n",
    "        f'<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">'\n",
    "        f'{\"\".join(url_generator)}'\n",
    "        f\"</urlset>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import responses\n",
    "import unittest\n",
    "from unittest.mock import patch, call\n",
    "\n",
    "\n",
    "class TestPrototype(unittest.TestCase):\n",
    "    @responses.activate\n",
    "    def test_scrape_sitemaps(self):\n",
    "        responses.get(\n",
    "            url=f\"{ROOT}s/1.xml\",\n",
    "            status=200,\n",
    "            body=create_template_sitemap([\"dark-summit\", \"warhawk\", \"dark-summit\"]),\n",
    "        )\n",
    "        responses.get(\n",
    "            url=f\"{ROOT}s/1.xml\",\n",
    "            status=200,\n",
    "            body=create_template_sitemap([\"\"]),\n",
    "        )\n",
    "        responses.get(\n",
    "            url=f\"{ROOT}s/1.xml\",\n",
    "            status=404,\n",
    "            body=create_template_sitemap([\"dark-summit\", \"warhawk\", \"dark-summit\"]),\n",
    "        )\n",
    "        responses.get(\n",
    "            url=f\"{ROOT}s/2.xml\",\n",
    "            status=200,\n",
    "            body=create_template_sitemap([\"warhawk\", \"metal-slug-2\"]),\n",
    "        )\n",
    "        responses.get(\n",
    "            url=f\"{ROOT}s/3.xml\",\n",
    "            status=200,\n",
    "            body=create_template_sitemap(),\n",
    "        )\n",
    "        with requests.Session() as s:\n",
    "            self.assertCountEqual(\n",
    "                scrape_sitemaps(s), [\"dark-summit\", \"warhawk\", \"metal-slug-2\"]\n",
    "            )\n",
    "            self.assertCountEqual(scrape_sitemaps(s), [\"warhawk\", \"metal-slug-2\"])\n",
    "            with patch(\"builtins.print\") as mocked_print:\n",
    "                self.assertCountEqual(scrape_sitemaps(s), [\"warhawk\", \"metal-slug-2\"])\n",
    "                self.assertEqual(\n",
    "                    mocked_print.mock_calls,\n",
    "                    [call(\"Sitemap page 1 failed to respond, skipped.\")],\n",
    "                )\n",
    "\n",
    "\n",
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

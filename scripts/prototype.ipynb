{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "from typing import Final, Iterator, Sequence\n",
    "\n",
    "import _csv\n",
    "import polars as pl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from lxml import etree as et\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "ROOT: Final[str] = \"https://www.metacritic.com/game\"\n",
    "STR_LIST_DELIM: Final[str] = \"|\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitemapsNotFoundError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class IndexNotIntegerError(Exception):\n",
    "    def __init__(self, msg: str = \"Default Message\") -> None:\n",
    "        super().__init__(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_sitemap_index(s: requests.Session) -> int:\n",
    "    \"\"\"Return index value of the last sitemap in the sitemap catalog.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemap catalog over a persistent http connection.\n",
    "    \"\"\"\n",
    "    sitemap_catalog = s.get(f\"{ROOT}s.xml\")\n",
    "    sitemap_catalog.raise_for_status()\n",
    "    catalog_etree = et.XML(sitemap_catalog.text)\n",
    "    nsmap = {\"ns\": catalog_etree.nsmap[None]}\n",
    "    try:\n",
    "        # XPath query tested faster compared to ElementPath methods\n",
    "        (last_url,) = catalog_etree.xpath(\n",
    "            \"(//ns:loc/text())[last()]\",\n",
    "            namespaces=nsmap,\n",
    "            smart_strings=False,\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        raise SitemapsNotFoundError from e\n",
    "    last_index_str = last_url.rpartition(\".\")[0].rpartition(\"/\")[-1]\n",
    "    try:\n",
    "        last_index = int(last_index_str)\n",
    "    except ValueError as e:\n",
    "        raise IndexNotIntegerError(f\"Last Index found to be: {last_index_str}\") from e\n",
    "    return last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_slugs(loc_elements: Iterator[et._Element], url_slugs: set[str]) -> None:\n",
    "    \"\"\"Process a single sitemap's loc tags and update the set of URL slugs.\n",
    "\n",
    "    Args:\n",
    "        loc_elements: A generator of loc tags containing URL text.\n",
    "        url_slugs: The set in which game URL slugs are to be stored.\n",
    "    \"\"\"\n",
    "    for loc in loc_elements:\n",
    "        try:\n",
    "            url_slug = (\n",
    "                loc.text\n",
    "                .rstrip(\"/\")  # pyright: ignore[reportOptionalMemberAccess]\n",
    "                .rpartition(\"/\")[-1]\n",
    "            )  # fmt: skip\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        url_slugs.add(url_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sitemaps(s: requests.Session, last_index: int) -> list[str]:\n",
    "    \"\"\"Retrieve URL slugs associated with all games from sitemaps.\n",
    "\n",
    "    Colloquially, a slug is the unique identifying part of a web address,\n",
    "    typically at the end of the URL. Each sitemap contains approximately 1000 URLs.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemaps over a persistent http connection.\n",
    "        last_index: End value for iteration of all sitemaps. (There appear to\n",
    "            be more unlisted sitemaps of higher indices in the catalog,\n",
    "            but these appear to be duplicates or vestigial in nature).\n",
    "\n",
    "    Returns:\n",
    "        A list of URL slugs corresponding to every game indexed in metacritic's sitemaps.\n",
    "    \"\"\"\n",
    "    url_slugs: set[str] = set()\n",
    "    for i in range(1, last_index + 1):\n",
    "        sitemap_response = s.get(f\"{ROOT}s/{i}.xml\")\n",
    "        sitemap_response.raise_for_status()\n",
    "        sitemap_etree = et.XML(sitemap_response.text)\n",
    "        loc_elements = sitemap_etree.iterfind(\".//loc\", namespaces=sitemap_etree.nsmap)\n",
    "        extract_url_slugs(loc_elements, url_slugs)\n",
    "        print(f\"Scraping of sitemap {i} complete\", end=\"\")\n",
    "        print(\"\\r\", end=\"\")\n",
    "\n",
    "    return list(url_slugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_game_details(\n",
    "    game_pg: requests.Response,\n",
    "    game_writer: _csv._writer,\n",
    "    game_dict: dict[str, str],\n",
    "    game_url: str,\n",
    ") -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    soup = BeautifulSoup(game_pg.content, features=\"lxml\")\n",
    "    processed_details: list[str | list[str]] = []\n",
    "    for key, selection_str in game_dict.items():\n",
    "        tag = soup.select_one(selection_str)\n",
    "        try:\n",
    "            if key == \"Summary\":\n",
    "                # Appending urls here means summary is last in csv, improving readability\n",
    "                processed_details.append(game_url)\n",
    "                # Special case as game summary text is only found in full as an attribute value\n",
    "                summary = (             # pyright: ignore[reportUnknownVariableType]\n",
    "                    tag                 # pyright: ignore[reportUnknownMemberType]\n",
    "                    .get(\"content\")     # pyright: ignore[reportOptionalMemberAccess]\n",
    "                    .replace(\"\\t\", \"\")  # pyright: ignore[reportOptionalMemberAccess, reportAttributeAccessIssue]\n",
    "                    .replace(\"\\n\", \" \")\n",
    "                    .strip()\n",
    "                )  # fmt:skip\n",
    "                processed_details.append(summary)  # pyright: ignore[reportUnknownArgumentType] # fmt: skip\n",
    "            elif key in [\"Platforms\", \"Developers\", \"Genres\"]:\n",
    "                processed_details.append(\n",
    "                    STR_LIST_DELIM.join((li.get_text(strip=True) for li in (tag or [])))\n",
    "                )\n",
    "            else:\n",
    "                stripped = tag.get_text(strip=True)  # pyright: ignore[reportOptionalMemberAccess] # fmt: skip\n",
    "                if key == \"Userscore\":\n",
    "                    try:\n",
    "                        # Scale Userscore to int range for later memory optimization\n",
    "                        processed_details.append(str(int(10 * float(stripped))))\n",
    "                    except ValueError:\n",
    "                        processed_details.append(stripped)\n",
    "                elif key == \"ESRB\":\n",
    "                    # Remove unnecessary \"Rated \" text preceding esrb label\n",
    "                    processed_details.append(stripped.partition(\" \")[-1])\n",
    "                else:\n",
    "                    processed_details.append(stripped)\n",
    "        except AttributeError:\n",
    "            processed_details.append(\"\")\n",
    "            continue\n",
    "    game_writer.writerow(processed_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_info(critic_pg: requests.Response) -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_games(s: requests.Session, url_slugs: list[str]) -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    CSS_SELECTORS: Final[dict[str, str]] = {\n",
    "        \"Title\":        \".c-productHero_title > div\",\n",
    "        \"Metascore\":    \".c-productHero_scoreInfo > div:first-child .c-siteReviewScore > span\",\n",
    "        \"Userscore\":    \".c-productHero_scoreInfo > div:last-child .c-siteReviewScore > span\",\n",
    "        \"ESRB\":         \".c-productionDetailsGame_esrb_title > span\",\n",
    "        \"Platforms\":    \".c-gameDetails_Platforms > ul\",\n",
    "        \"Release_Date\": \".c-gameDetails_ReleaseDate > span:last-child\",\n",
    "        \"Developers\":   \".c-gameDetails_Developer > ul\",\n",
    "        \"Publisher\":    \".c-gameDetails_Distributor > span:last-child\",\n",
    "        \"Genres\":       \"ul.c-genreList\",\n",
    "        \"Summary\":      \"meta[name='description']\",\n",
    "    }  # fmt: skip\n",
    "\n",
    "    with open(\"../data/games_backup.tsv\", \"w\", newline=\"\") as game_file:\n",
    "        game_writer = csv.writer(game_file, delimiter=\"\\t\")\n",
    "        game_headers = list(CSS_SELECTORS.keys())\n",
    "        game_headers.insert(-1, \"Game_Url\")\n",
    "        game_writer.writerow(game_headers)\n",
    "        for url_slug in url_slugs:\n",
    "            game_url = f\"{ROOT}/{url_slug}\"\n",
    "            game_pg = s.get(game_url)\n",
    "            if not game_pg.ok and game_pg.status_code != 429:\n",
    "                print(\"Error fetching game page\")\n",
    "                continue\n",
    "            # critic_pg = s.get(f\"{game_url}/critic-reviews\")\n",
    "            scrape_game_details(game_pg, game_writer, CSS_SELECTORS, game_url)\n",
    "            # scrape_critic_info(critic_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SUMMARY = (\n",
    "    \"Metacritic aggregates music, game, tv, and movie reviews from the leading critics. \"\n",
    "    \"Only Metacritic.com uses METASCORES, which let you know at a glance how each item was reviewed.\"\n",
    ")\n",
    "dtype_dict = {\n",
    "    \"Metascore\": pl.UInt8,\n",
    "    \"Userscore\": pl.UInt8,\n",
    "    \"ESRB\": pl.Categorical,\n",
    "    \"Publisher\": pl.Categorical,\n",
    "}\n",
    "x = pl.read_csv(\n",
    "    \"../data/games_backup.tsv\",\n",
    "    separator=\"\\t\",\n",
    "    null_values=[\"tbd\", DEFAULT_SUMMARY],\n",
    "    dtypes=dtype_dict,\n",
    ")\n",
    "x = x.with_columns(\n",
    "    pl.col(\"Release_Date\").str.to_date(\"%b %d, %Y\"),\n",
    "    pl.col(\"Platforms\").str.split(STR_LIST_DELIM),\n",
    "    pl.col(\"Developers\").str.split(STR_LIST_DELIM),\n",
    "    pl.col(\"Genres\").str.split(STR_LIST_DELIM),\n",
    ")\n",
    "x.explode(\"Genres\").explode(\"Developers\").explode(\"Platforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SLUGS = [\n",
    "    \"bioshock\",\n",
    "    \"tilescape\",\n",
    "    \"cieb-the-backrooms-project\",\n",
    "    \"elden-ring\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(\n",
    "    *,\n",
    "    user_agent: str = \"Edge\",\n",
    "    num_retries: int = 5,\n",
    "    backoff_factor: float = 0.5,\n",
    "    backoff_jitter: float = 0.5,\n",
    "    status_forcelist: list[int] = [429],\n",
    "    respect_retry_after_header: bool = False,\n",
    "    games_backup_path: str = \"../data/games_backup.tsv\",\n",
    "    slugs_backup_path: str = \"../data/raw_url_slugs.tsv\",\n",
    ") -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    with requests.Session() as s:\n",
    "        s.headers = {\"User-Agent\": user_agent}\n",
    "        retry_config = Retry(\n",
    "            total=num_retries,\n",
    "            backoff_factor=backoff_factor,\n",
    "            backoff_jitter=backoff_jitter,\n",
    "            status_forcelist=status_forcelist,\n",
    "            respect_retry_after_header=respect_retry_after_header,\n",
    "        )\n",
    "        s.mount(\"https://\", HTTPAdapter(max_retries=retry_config))\n",
    "        try:\n",
    "            games_backup = pl.read_csv(games_backup_path)\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                url_slugs = pl.read_csv(slugs_backup_path).to_series().to_list()\n",
    "            except FileNotFoundError:\n",
    "                print(\"Missing url slugs backup file...\\nPreparing to rebuild\")\n",
    "                last_index = get_last_sitemap_index(s)\n",
    "                print(\n",
    "                    \"Last sitemap index found, preparing to iterate over all catalogued sitemaps\"\n",
    "                )\n",
    "                url_slugs = scrape_sitemaps(s, last_index)\n",
    "                pl.DataFrame({\"url_slugs\": url_slugs}).write_csv(\n",
    "                    file=slugs_backup_path,\n",
    "                    separator=\"\\t\",\n",
    "                )\n",
    "                print(\"Url slugs backup rebuilt successfully\")\n",
    "            scrape_games(s, TEST_SLUGS)\n",
    "\n",
    "\n",
    "scraper(games_backup_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(\n",
    "    url_slugs: Sequence[str | int] = [], *, is_index: bool = False\n",
    ") -> str:\n",
    "    \"\"\"Return string adhereing to XML schema for the Sitemap protocol\n",
    "    containing user defined dummy urls.\n",
    "\n",
    "    Utility function to construct body content for mock http responses.\n",
    "    Any type of string is accepted, but only ints or int-like strings are\n",
    "    expected within the list if is_index is true (constructing a sitemap index).\n",
    "\n",
    "    Args:\n",
    "        url_slugs: Corresponds to game titles when constructing a sitemap, while\n",
    "            represents ordinal values when constructing a sitemap index.\n",
    "        is_index: Determines whether to construct a template for a sitemap\n",
    "            or a sitemap index.\n",
    "    \"\"\"\n",
    "    ns = 'xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"'\n",
    "    container_start_tag = f\"<urlset {ns}>\"\n",
    "    item_start_tag = \"<url><loc>\"\n",
    "    ext = \"/\"\n",
    "    item_end_tag = \"</loc></url>\"\n",
    "    container_end_tag = \"</urlset>\"\n",
    "    if is_index:\n",
    "        container_start_tag = f\"<sitemapindex {ns}>\"\n",
    "        item_start_tag = \"<sitemap><loc>\"\n",
    "        ext = \".xml\"\n",
    "        item_end_tag = \"</loc></sitemap>\"\n",
    "        container_end_tag = \"</sitemapindex>\"\n",
    "    # The empty string element is a special case where the url root is also omitted.\n",
    "    url_generator = (\n",
    "        (\n",
    "            f\"{item_start_tag}{item_end_tag}\"\n",
    "            if slug == \"\"\n",
    "            else f\"{item_start_tag}{ROOT}/{slug}{ext}{item_end_tag}\"\n",
    "        )\n",
    "        for slug in url_slugs\n",
    "    )\n",
    "    return f\"{container_start_tag}{\" \".join(url_generator)}{container_end_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "import responses\n",
    "\n",
    "\n",
    "class TestPrototype(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.s = cls.enterClassContext(requests.Session())\n",
    "        cls.r = cls.enterClassContext(responses.RequestsMock())\n",
    "\n",
    "    def test_get_last_sitemap_index(self) -> None:\n",
    "        statuses = [200] * 5 + [404]\n",
    "        slugs_to_test = [\n",
    "            [7],\n",
    "            [\"a\", 20],\n",
    "            [11, 3, 94],\n",
    "            [],\n",
    "            [20, \"a\"],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        for status, slugs in zip(statuses, slugs_to_test, strict=True):\n",
    "            self.r.get(\n",
    "                url=f\"{ROOT}s.xml\",\n",
    "                status=status,\n",
    "                body=create_template(slugs, is_index=True),\n",
    "            )\n",
    "\n",
    "        for i in range(3):\n",
    "            self.assertEqual(slugs_to_test[i][-1], get_last_sitemap_index(self.s))\n",
    "        self.assertRaises(SitemapsNotFoundError, get_last_sitemap_index, self.s)\n",
    "        self.assertRaises(IndexNotIntegerError, get_last_sitemap_index, self.s)\n",
    "        self.assertRaises(requests.HTTPError, get_last_sitemap_index, self.s)\n",
    "\n",
    "    def test_scrape_sitemaps(self) -> None:\n",
    "        statuses = [200, 200, 404]\n",
    "        slugs_to_test = [\n",
    "            [\"dark-summit\", \"warhawk\", \"dark-summit\"],\n",
    "            [\"\"],\n",
    "            [\"bioshock\"],\n",
    "        ]\n",
    "        for status, slugs in zip(statuses, slugs_to_test, strict=True):\n",
    "            self.r.get(\n",
    "                url=f\"{ROOT}s/1.xml\",\n",
    "                status=status,\n",
    "                body=create_template(slugs),\n",
    "            )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s/2.xml\",\n",
    "            status=200,\n",
    "            body=create_template([\"warhawk\", \"metal-slug-2\"]),\n",
    "        )\n",
    "\n",
    "        TEST_INDEX = 2\n",
    "        self.assertCountEqual(\n",
    "            [\"dark-summit\", \"warhawk\", \"metal-slug-2\"],\n",
    "            scrape_sitemaps(self.s, TEST_INDEX),\n",
    "        )\n",
    "        self.assertCountEqual(\n",
    "            [\"warhawk\", \"metal-slug-2\"],\n",
    "            scrape_sitemaps(self.s, TEST_INDEX),\n",
    "        )\n",
    "        self.assertRaises(requests.HTTPError, scrape_sitemaps, self.s, TEST_INDEX)\n",
    "\n",
    "\n",
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

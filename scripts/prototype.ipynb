{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import cchardet\n",
    "import lxml\n",
    "import os\n",
    "import polars as pl\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from lxml import etree as et\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from typing import Iterator\n",
    "\n",
    "ROOT = \"https://www.metacritic.com/game\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_sitemap_index(s: requests.Session) -> int:\n",
    "    \"\"\"Return index value of the last sitemap in the sitemap catalog.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemap catalog over a persistent http connection.\n",
    "    \"\"\"\n",
    "    sitemap_catalog = s.get(f\"{ROOT}s.xml\")\n",
    "    sitemap_catalog.raise_for_status()\n",
    "    catalog_etree = et.XML(sitemap_catalog.text)\n",
    "    nsmap = {\"ns\": catalog_etree.nsmap[None]}\n",
    "    # XPath query tested faster compared to ElementPath methods\n",
    "    last_url = catalog_etree.xpath(\n",
    "        \"(//ns:loc/text())[last()]\",\n",
    "        namespaces=nsmap,\n",
    "        smart_strings=False,\n",
    "    )\n",
    "    if not last_url:\n",
    "        raise IndexError(\"No sitemaps found.\")\n",
    "    last_index = int(last_url[0].rpartition(\"/\")[-1].partition(\".\")[0])\n",
    "    return last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_slugs(loc_elements: Iterator[et._Element], url_slugs: set[str]) -> None:\n",
    "    \"\"\"Process a single sitemap's loc tags and update the set of URL slugs.\n",
    "\n",
    "    Args:\n",
    "        loc_elements: A generator of loc tags containing URL text.\n",
    "        url_slugs: The set in which game URL slugs are to be stored.\n",
    "    \"\"\"\n",
    "    for loc in loc_elements:\n",
    "        if (url := loc.text) is None:\n",
    "            continue\n",
    "        url_slug = url.rstrip(\"/\").rpartition(\"/\")[-1]\n",
    "        url_slugs.add(url_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sitemaps(s: requests.Session, last_index: int) -> list[str]:\n",
    "    \"\"\"Retrieve URL slugs associated with all games from sitemaps.\n",
    "\n",
    "    Colloquially, a slug is the unique identifying part of a web address,\n",
    "    typically at the end of the URL. Each sitemap contains approximately 1000 URLs.\n",
    "\n",
    "    Args:\n",
    "        s: Session object to fetch sitemaps over a persistent http connection.\n",
    "        last_index: End value for iteration of all sitemaps. (There appear to\n",
    "            be more unlisted sitemaps of higher indices in the catalog,\n",
    "            but these appear to be duplicates or vestigial in nature).\n",
    "\n",
    "    Returns:\n",
    "        A list of URL slugs corresponding to every game indexed in metacritic's sitemaps.\n",
    "    \"\"\"\n",
    "    url_slugs: set[str] = set()\n",
    "    for i in range(1, last_index + 1):\n",
    "        sitemap_response = s.get(f\"{ROOT}s/{i}.xml\")\n",
    "        sitemap_response.raise_for_status()\n",
    "        sitemap_etree = et.XML(sitemap_response.text)\n",
    "        loc_elements = sitemap_etree.iterfind(\".//loc\", namespaces=sitemap_etree.nsmap)\n",
    "        extract_url_slugs(loc_elements, url_slugs)\n",
    "\n",
    "    return list(url_slugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_games(s: requests.Session, url_slugs: list[str]) -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(\n",
    "    root_url: str,\n",
    "    user_agent: str = \"Edge\",\n",
    "    max_retries: int = 5,\n",
    ") -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    with requests.Session() as s:\n",
    "        s.headers = {\"User-Agent\": user_agent}\n",
    "        retry_config = Retry(\n",
    "            total=max_retries,\n",
    "            backoff_factor=0.5,\n",
    "            backoff_jitter=0.5,\n",
    "            status_forcelist=[429],\n",
    "            respect_retry_after_header=False,\n",
    "        )\n",
    "        s.mount(\"https://\", HTTPAdapter(max_retries=retry_config))\n",
    "        if not os.path.isfile(\"../data/games.tsv\"):\n",
    "            last_index = get_last_sitemap_index(s)\n",
    "            url_slugs = scrape_sitemaps(s, last_index)\n",
    "            pl.DataFrame({\"url_slugs\": url_slugs}).write_csv(\n",
    "                file=\"../data/games.tsv\",\n",
    "                separator=\"\\t\",\n",
    "            )\n",
    "        else:\n",
    "            url_slugs = pl.read_csv(\"../data/games.tsv\").to_series().to_list()\n",
    "        scrape_games(s, url_slugs)\n",
    "\n",
    "\n",
    "scraper(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(url_slugs: list[str | int] = [], *, isIndex: bool = False) -> str:\n",
    "    \"\"\"Return string adhereing to XML schema for the Sitemap protocol\n",
    "    containing user defined dummy urls.\n",
    "\n",
    "    Utility function to construct body content for mock http responses.\n",
    "    Any type of string is accepted, but only ints or int-like strings are\n",
    "    expected within the list if isIndex is true (constructing a sitemap index).\n",
    "\n",
    "    Args:\n",
    "        url_slugs: Corresponds to game titles when constructing a sitemap, while\n",
    "            represents ordinal values when constructing a sitemap index.\n",
    "        isIndex: Determines whether to construct a template for a sitemap\n",
    "            or a sitemap index.\n",
    "    \"\"\"\n",
    "    ns = 'xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"'\n",
    "    container_start_tag = f\"<urlset {ns}>\"\n",
    "    item_start_tag = \"<url><loc>\"\n",
    "    ext = \"/\"\n",
    "    item_end_tag = \"</loc></url>\"\n",
    "    container_end_tag = \"</urlset>\"\n",
    "    if isIndex:\n",
    "        container_start_tag = f\"<sitemapindex {ns}>\"\n",
    "        item_start_tag = \"<sitemap><loc>\"\n",
    "        ext = \".xml\"\n",
    "        item_end_tag = \"</loc></sitemap>\"\n",
    "        container_end_tag = \"</sitemapindex>\"\n",
    "    # The empty string element is a special case where the url root is also omitted.\n",
    "    url_generator = (\n",
    "        (\n",
    "            f\"{item_start_tag}{item_end_tag}\"\n",
    "            if slug == \"\"\n",
    "            else f\"{item_start_tag}{ROOT}/{slug}{ext}{item_end_tag}\"\n",
    "        )\n",
    "        for slug in url_slugs\n",
    "    )\n",
    "    return f\"{container_start_tag}{\" \".join(url_generator)}{container_end_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import responses\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestPrototype(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.s = cls.enterClassContext(requests.Session())\n",
    "        cls.r = cls.enterClassContext(responses.RequestsMock())\n",
    "\n",
    "    def test_get_last_sitemap_index(self) -> None:\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s.xml\",\n",
    "            status=200,\n",
    "            body=create_template([7], isIndex=True),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s.xml\",\n",
    "            status=200,\n",
    "            body=create_template([\"a\", 20], isIndex=True),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s.xml\",\n",
    "            status=200,\n",
    "            body=create_template([11, 3, 94], isIndex=True),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s.xml\",\n",
    "            status=404,\n",
    "            body=create_template([0, 0], isIndex=True),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s.xml\",\n",
    "            status=200,\n",
    "            body=create_template(isIndex=True),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s.xml\",\n",
    "            status=200,\n",
    "            body=create_template([20, \"a\"], isIndex=True),\n",
    "        )\n",
    "        self.assertEqual(get_last_sitemap_index(self.s), 7)\n",
    "        self.assertEqual(get_last_sitemap_index(self.s), 20)\n",
    "        self.assertEqual(get_last_sitemap_index(self.s), 94)\n",
    "        self.assertRaises(requests.HTTPError, get_last_sitemap_index, self.s)\n",
    "        self.assertRaisesRegex(\n",
    "            IndexError, \"No sitemaps found.\", get_last_sitemap_index, self.s\n",
    "        )\n",
    "        self.assertRaises(ValueError, get_last_sitemap_index, self.s)\n",
    "\n",
    "    def test_scrape_sitemaps(self) -> None:\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s/1.xml\",\n",
    "            status=200,\n",
    "            body=create_template([\"dark-summit\", \"warhawk\", \"dark-summit\"]),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s/1.xml\",\n",
    "            status=200,\n",
    "            body=create_template([\"\"]),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s/1.xml\",\n",
    "            status=404,\n",
    "            body=create_template([\"dark-summit\", \"warhawk\", \"dark-summit\"]),\n",
    "        )\n",
    "        self.r.get(\n",
    "            url=f\"{ROOT}s/2.xml\",\n",
    "            status=200,\n",
    "            body=create_template([\"warhawk\", \"metal-slug-2\"]),\n",
    "        )\n",
    "        TEST_INDEX = 2\n",
    "        self.assertCountEqual(\n",
    "            scrape_sitemaps(self.s, TEST_INDEX),\n",
    "            [\"dark-summit\", \"warhawk\", \"metal-slug-2\"],\n",
    "        )\n",
    "        self.assertCountEqual(\n",
    "            scrape_sitemaps(self.s, TEST_INDEX),\n",
    "            [\"warhawk\", \"metal-slug-2\"],\n",
    "        )\n",
    "        self.assertRaises(requests.HTTPError, scrape_sitemaps, self.s, TEST_INDEX)\n",
    "\n",
    "\n",
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
